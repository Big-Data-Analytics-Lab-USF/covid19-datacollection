
________________________________________________________ USC DATA COLLECTION ______________________________________________________________________

USC DATA COLLECTION  

Process of Hydration: Because our work needed to be quick, we chose to use application. Whereas we only got .jsonl (jsonLines) files from the script version of the Hydrator, we got both .csv & .jsonl from the Hydrator software, which gave us some momentum when we needed to run botometer scripts on usernames. In addition to that, script version of the software needed special libraries which required pre-approvals from CIRCE. And because our personal computer lack memory, we went with application which made sense to us at the time.  

Software: can be found here (https://github.com/DocNow/hydrator). 

  

Short Comings: Some of the .jsonl (jsonLines) files had inappropriate EOFs markers so we had to recreate all the .jsonl files. Furthermore, at the time, their Linux version of the software was impossible to run on our CIRCE server thus Tuc, Dre & Mihir had to rely on their Windows OS to run each tweet IDs text files from the USC (https://github.com/echen102/COVID-19-TweetIDs) 

In reality we didnâ€™t start collecting the data (tweet Ids which USC provided) until May. Therefore, we had heavy loss of data since Twitter was actively deleting tweets (linked to mis & dis information). In January, February, and March we noticed saw couple of millions of tweets, however as we hydrated April and May, we had some healthy amount of data. 

        USF Script: Dre upgraded Dr Scott Hale's script (https://github.com/computermacgyver/twitter-python) in order to stream twitter data continuously. Our script is located at (https://github.com/Big-Data-Analytics-Lab-USF/covid19-datacollection). 

  

Data Collected:  Our (USF) approach was hashtags based, we were getting tweets from every topic. However, we liked the USC data and pursue onto collecting it because it was keywords based. We later, specifically in May, adapted their technique. Therefore, we used their dataset to double check what we've collected from March till May. 

        January: 01/21-01/31 (Acquistion: 05/07-05/09) 

        February: 02/1-02-29 (Acquistion: 05/07-05/11) 

        March: 03/1-03/31 (Acquistion: 05/15-05/17) 

        April: 04/1-04/31 (Acquistion: 05/18-05/22) 

        May: 05/1-05/22 (Acquistion: 05/23-05/26) 

NOTE:
	Twitter data always uses GMT (from what I can tell), so we can save other people time later.
